---
title: "Week 5: Bayesian linear regression and introduction to Stan"
date: today
date-format: "DD/MM/YY"
format: pdf
execute: 
  warning: false
  message: false
---

# Introduction

Today we will be starting off using Stan, looking at the kid's test score data set (available in resources for the [Gelman Hill textbook](https://mc-stan.org/rstanarm/reference/rstanarm-datasets.html)). 

```{r}
library(tidyverse)
library(rstan)
library(tidybayes)
library(here)
```


The data look like this:

```{r}
kidiq <- read_rds(here("kidiq.RDS"))
kidiq
```
As well as the kid's test scores, we have a binary variable indicating whether or not the mother completed high school, the mother's IQ and age. 


# Descriptives

## Question 1

Use plots or tables to show three interesting observations about the data. Remember:

- Explain what your graph/ tables show
- Choose a graph type that's appropriate to the data type


```{r}
library(ggplot2)

ggplot(kidiq, aes(x = mom_iq, y = kid_score)) +
  geom_point(alpha = 0.6) +   
  labs(
    title = "Child's Test Score vs Mothers IQ",
    x = "Mother's IQ",
    y = "Child's Test Score"
  )

```

This scatter plot shows the relationship between mothers IQ and child's test score. It appears from this plot that there is a positive correlation between the two.


```{r}

ggplot(kidiq, aes(x = factor(mom_hs), y = kid_score, fill = factor(mom_hs))) +
  geom_boxplot() +
  labs(
    title = "Child Test Scores by Mother's Education ",
    x = "High School (0=No, 1=Yes)",
    y = "Child's Test Score",
    fill = "HS Completion"
  )


```

This boxplot shows the difference in child's test scores by whether the mother has completed high school or not. It is noticeable that the median score is lower for no high school completion and that there seems to be a wider spread/more variance in that group.


```{r}
kidiq |>
  ggplot(aes(x = kid_score)) + 
  geom_histogram(binwidth = 5, color = "white", alpha = 0.6) + 
  labs(
    title = "Distribution of Child Test Scores",
    x = "Child's Test Score",
    y = "Count"
  ) +
  theme_bw()


```

This histogram shows the distribution of child's test scores. The scores seem to be somewhat centered around 100, however, the scores have a very heavy left tail and a steep drop off to the right of 100. We also see potential outliers near 0 and 150, on both ends of the distribution.

# Estimating mean, no covariates

In class we were trying to estimate the mean and standard deviation of the kid's test scores. The `kids2.stan` file contains a Stan model to do this. If you look at it, you will notice the first `data` chunk lists some inputs that we have to define: the outcome variable `y`, number of observations `N`, and the mean and standard deviation of the prior on `mu`. Let's define all these values in a `data` list.

$$
y_i |\mu, \sigma \sim N(\mu, \sigma^2)
$$
priors:
$$
\sigma \sim N^+(0,10^2)
$$
$$
\mu \sim N(\mu_0, \sigma^2_0)
$$


```{r}
y <- kidiq$kid_score
mu0 <- 80
sigma0 <- 10

# named list to input for stan function
data <- list(y = y, 
             N = length(y), 
             mu0 = mu0,
             sigma0 = sigma0)
```



Now we can run the model:

```{r}
fit <- stan(file = here("kids2.stan"),
            data = data,
            # reducing the iterations a bit to speed things up
            chains = 3,
            iter = 500)
```

Look at the summary

```{r}
fit
```

Traceplot

```{r}
traceplot(fit)
```

All looks fine. 

```{r}
pairs(fit, pars = c("mu", "sigma"))
```

```{r}
stan_dens(fit, separate_chains = TRUE)
```


## Understanding output

What does the model actually give us? A number of samples from the posteriors. To see this, we can use `extract` to get the samples. 

```{r}
post_samples <- extract(fit)
names(post_samples)
head(post_samples[["mu"]])
```


This is a list, and in this case, each element of the list has 4000 samples. E.g. quickly plot a histogram of mu

```{r}
hist(post_samples[["mu"]])
median(post_samples[["mu"]])
# 95% bayesian credible interval
quantile(post_samples[["mu"]], 0.025)
quantile(post_samples[["mu"]], 0.975)
```

Tidybayes is also very useful:

```{r}
fit |> 
  gather_draws(mu, sigma) |> 
  median_qi(.width = 0.8)
```


## Plot estimates

There are a bunch of packages, built-in functions that let you plot the estimates from the model, and I encourage you to explore these options (particularly in `bayesplot`, which we will most likely be using later on). I like using the `tidybayes` package, which allows us to easily get the posterior samples in a tidy format (e.g. using gather draws to get in long format). Once we have that, it's easy to just pipe and do ggplots as usual. 


Get the posterior samples for mu and sigma in long format:

```{r}
dsamples <- fit  |> 
  gather_draws(mu, sigma) # gather = long format
dsamples

# wide format
fit  |>  spread_draws(mu, sigma)

# quickly calculate the quantiles using 

dsamples |> 
  median_qi(.width = 0.8)
```

Let's plot the density of the posterior samples for mu and add in the prior distribution

```{r}
dsamples |> 
  filter(.variable == "mu") |> 
  ggplot(aes(.value, color = "posterior")) + geom_density(size = 1) + 
  xlim(c(70, 100)) + 
  stat_function(fun = dnorm, 
        args = list(mean = mu0, 
                    sd = sigma0), 
        aes(colour = 'prior'), size = 1) +
  scale_color_manual(name = "", values = c("prior" = "red", "posterior" = "black")) + 
  ggtitle("Prior and posterior for mean test scores") + 
  xlab("score")
  
```

## Question 2

Change the prior to be much more informative (by changing the standard deviation to be 0.1). Rerun the model. Do the estimates change? Plot the prior and posterior densities. 


```{r}

y <- kidiq$kid_score
mu0 <- 80
sigma0 <- 0.1

# named list to input for stan function
data <- list(y = y, 
             N = length(y), 
             mu0 = mu0,
             sigma0 = sigma0)

fit_v2 <- stan(file = here("kids2.stan"),
            data = data,
            # reducing the iterations a bit to speed things up
            chains = 3,
            iter = 500)
```

```{r}

dsamples_v2 <- fit_v2  |> 
  gather_draws(mu, sigma) # gather = long format
dsamples_v2

# quickly calculate the quantiles using 

dsamples_v2 |> 
  median_qi(.width = 0.8)
```


The estimates have now changed quite a bit. Mu has gone from 86 to 80 with a much tighter lower and upper bound.

```{r}
dsamples_v2 |> 
  filter(.variable == "mu") |> 
  ggplot(aes(.value, color = "posterior")) + geom_density(size = 1) + 
  xlim(c(70, 100)) + 
  stat_function(fun = dnorm, 
        args = list(mean = mu0, 
                    sd = sigma0), 
        aes(colour = 'prior'), size = 1) +
  scale_color_manual(name = "", values = c("prior" = "red", "posterior" = "black")) + 
  ggtitle("Prior and posterior for mean test scores") + 
  xlab("score")

```
From the plots we can see why this happens. Instead of having a very smooth and flat prior around 80, we instead have a very narrow distribution about 80 which the data is not able to overcome and becomes very narrow about 80 as well. With the previous prior, the data had the most influence and we got a more spread distribution about 86.


# Adding covariates

Now let's see how kid's test scores are related to mother's education. We want to run the simple linear regression

$$
y_i|\mu_i, \sigma^2 \sim N(\mu_i, \sigma^2)
$$



$$
\mu_i = \alpha + \beta X_i
$$
Priors:
$$
\alpha \sim N(0, 100^2)
$$
$$
\beta\sim N(0, 10^2)
$$
$$
\sigma \sim N(0, 10^2)
$$


where $X = 1$ if the mother finished high school and zero otherwise. 

`kid3.stan` has the stan model to do this. Notice now we have some inputs related to the design matrix $X$ and the number of covariates (in this case, it's just 1).

Let's get the data we need and run the model. 



```{r}
X <- as.matrix(kidiq$mom_hs, ncol = 1) # force this to be a matrix
K <- 1

data <- list(y = y, N = length(y), 
             X =X, K = K)
fit2 <- stan(file = here("kids3.stan"),
            data = data, 
            iter = 1000)

fit2
traceplot(fit2)
```

## Question 3

a) Confirm that the estimates of the intercept and slope are comparable to results from `lm()` 


```{r}

df <- as.data.frame(kidiq)

lm_fit3 <- lm(kid_score ~ mom_hs, data = df)

summary(lm_fit3)
```

The intercept and slope using lm()are comparable to the stan output. We see an intercept of 77.55 from lm() and 78.07 from stan and a slope of 11.77 from lm() and 11.10 from stan. They are not identical, but they are similar.

b) Do a `pairs` plot to investigate the joint sample distributions of the slope and intercept. Comment briefly on what you see. Is this potentially a problem?

```{r}
pairs(fit2, pars = c("alpha", "beta"))
```

From the pairs plot we see correlation between the slope and intercept. This is a problem as correlated parameters reduce Stan's efficiency.

## Plotting results

It might be nice to plot the posterior samples of the estimates for the non-high-school and high-school mothered kids. Here's some code that does this: notice the `beta[condition]` syntax. Also notice I'm using `spread_draws`, because it's easier to calculate the estimated effects in wide format

```{r}
fit2 |>
  spread_draws(alpha, beta[k], sigma) |> 
     mutate(nhs = alpha, # no high school is just the intercept
          hs = alpha + beta) |> 
  select(nhs, hs) |> 
  pivot_longer(nhs:hs, names_to = "education", values_to = "estimated_score") |> 
  ggplot(aes(y = education, x = estimated_score)) +
  stat_halfeye() + 
  theme_bw() + 
  ggtitle("Posterior estimates of scores by education level of mother")

```


```{r}

fit2 |>
  spread_draws(alpha, beta[k]) |>
  mutate(lp_nhs = alpha,
         lp_hs = alpha+beta) |>
  ungroup() |>
  select(lp_nhs:lp_hs) |>
  pivot_longer(lp_nhs:lp_hs) |>
  ggplot(aes(value, fill = name)) +
  geom_density()

```

## Question 4

Add in mother's IQ as a covariate and rerun the model. Please mean center the covariate before putting it into the model. Interpret the coefficient on the (centered) mum's IQ. 

```{r}
X <- cbind(kidiq$mom_hs, (kidiq$mom_iq - mean(kidiq$mom_iq)))
data <- list(y = y,
             N = length(y),
             X = X,
             K = 2)

fit3 <- stan(file = "kids3.stan",
             data = data,
             iter = 500)

fit3
```

We can interpret the coefficient for mothers IQ as follows: for a one score increase in mothers IQ from the mean IQ, the child's expected test score increases by 0.56 points, while holding the mothers high school status fixed in the model.

## Question 5 

Confirm the results from Stan agree with `lm()`


```{r}
df <- tibble(y = y, hs = X[,1], iq = X[,2])
summary(lm(y ~ hs + iq, data = df))
```

Again, we see very similar results between stan and lm(). Intercept is 82.12 using lm(), 82.20 in stan. The coefficient for hs using 5.95 in lm() and 5.82 in stan. The coefficient for iq is 0.56 using lm() and 0.56 in stan.


## Question 6

Plot the posterior estimates of scores by education of mother for mothers who have an IQ of 110. 


```{r}
fit3 |>
  spread_draws(alpha, beta[k]) |>
  mutate(k = ifelse(k == 1, "hs", "iq")) |>
  pivot_wider(names_from = "k", values_from = "beta") |>
  mutate(lp_nhs = alpha + iq*(110 - mean(kidiq$mom_iq)),
         lp_hs = alpha + hs + iq*(110 - mean(kidiq$mom_iq))) |>
  select(lp_nhs:lp_hs) |>
  pivot_longer(lp_nhs:lp_hs, names_to = "education", values_to = "estimated_score") |>
  ggplot(aes(y = education, x = estimated_score)) +
  stat_halfeye() + 
  theme_bw() + 
  ggtitle("Posterior estimates of scores by education level of mother",
          subtitle = "For mothers with IQ = 100")


```

## Question 7

Generate and plot (as a histogram) samples from the posterior predictive distribution for a new kid with a mother who graduated high school and has an IQ of 95. 

```{r}
res <- fit3 |> 
  spread_draws(alpha, beta[k], sigma) |>
  pivot_wider(names_from = "k", values_from = "beta") |>
  mutate(mu = alpha + `1` + `2`*(95 - mean(kidiq$mom_iq)))

y_tilde <- rnorm(n = nrow(res), mean = res$mu, res$sigma)

```

```{r}
as_tibble(y_tilde) |>
  ggplot(aes(value)) +
  geom_histogram(fill = "navy", color = "pink")

```
```{r}
tibble(y_tilde = y_tilde, mu = res$mu) |>
  pivot_longer(y_tilde:mu) |>
  ggplot(aes(value, fill = name)) +
  geom_histogram(position = "dodge", bins = 100)

```


